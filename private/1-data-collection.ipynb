{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Business of Apps allowed this project to be possible by providing a dataset of ...\n",
    "\n",
    "### Web Scraping Goals (revisar)\n",
    "\n",
    "0. Configure web scraping system;\t\n",
    "1. Reach target pages;\n",
    "2. Locate the charts;\n",
    "3. Identify each chart's title;\n",
    "4. Construct iframe selectors using the chart titles;\n",
    "5. Save the iframe selectors in a list;\n",
    "6. Use the iframe list to perform actions within them;\n",
    "7. Hover over each chart column with the mouse pointer;\n",
    "8. Save the values that appear in the page source when hovering over each chart column.\n",
    "\n",
    "### Raw Data Preparation\n",
    "\n",
    "### Data Version Control\n",
    "\n",
    "Para garantir a reprodutibilidade dessa etapa do experimento, desenvolvi um sistema simples de controle de versão de dados (DVC) Ad-Hoc. \n",
    "\n",
    "Esse sistema permitiu a comparação automática dos dados coletados em execuções futuras com os dados obtidos anteriormente, ou seja, com os dados que já haviam sido armazenados no estudo.\n",
    "Apos os dados terem sido coletados, eles foram registrados como a primeira versão dos dados, então salvos dentro do repositório e no bloco de codigo em uma estrutura de dados apropriada. \n",
    "\n",
    "Novos dados coletados são comparados com essa versão dos dados, quando o jupyter notebook é executado novamente, e se houver diferenças um aviso é lançado no output e pipelinee os novos dados são salvos como a nova versão dos dados automaticamente. Em caso de serem identicos, então o sistema não faz nada e o pipeline é interrompido.\n",
    "\n",
    "### Reproducibility Note\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from IPython.display import Image, display\n",
    "from tqdm.asyncio import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "targets_url = ['https://www.businessofapps.com/data/instagram-statistics/', \n",
    "               'https://www.businessofapps.com/data/tik-tok-statistics/']\n",
    "target_data_selector = 'infogram-embed'\n",
    "target_data_iframe_selector = 'iframe[title=\"{chart_iframe_title_goes_here}\"]'\n",
    "\n",
    "p = await async_playwright().start()\n",
    "browser = await p.chromium.launch()\n",
    "context = await browser.new_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web scraping system was successfully configured. The results were:\n",
    "\n",
    "- All web scraping parameters were set up;\n",
    "- Playwright API was successfully initialized.\n",
    "\n",
    "Web scraping goals reached:\n",
    "\n",
    "1. ~~Configure web scraping stack;~~\t\n",
    "2. Reach target pages;\n",
    "3. Locate the charts;\n",
    "4. Identify each chart's title;\n",
    "5. Construct iframe selectors using the chart titles;\n",
    "6. Save the iframe selectors in a list;\n",
    "7. Use the iframe list to perform actions within them;\n",
    "8. Hover over each chart column with the mouse pointer;\n",
    "9. Scrape the values that appear in the page source when hovering over each chart column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Pages Reacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = await context.new_page()\n",
    "tk = await context.new_page()\n",
    "\n",
    "await ig.goto(targets_url[0])\n",
    "print(await ig.title())\n",
    "display(Image(await ig.screenshot()))\n",
    "\n",
    "await tk.goto(targets_url[1])\n",
    "print(await tk.title())\n",
    "display(Image(await tk.screenshot()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web scraping system was successfully initiated. The results were:\n",
    "- Target URLs was successfully reached.\n",
    "\n",
    "Web scraping goals reached:\n",
    "\n",
    "1. ~~Configure web scraping stack;~~\t\n",
    "2. ~~Reach target pages;~~\n",
    "3. Locate the charts;\n",
    "4. Identify each chart's title;\n",
    "5. Construct iframe selectors using the chart titles;\n",
    "6. Save the iframe selectors in a list;\n",
    "7. Use the iframe list to perform actions within them;\n",
    "8. Hover over each chart column with the mouse pointer;\n",
    "9. Scrape the values that appear in the page source when hovering over each chart column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data (charts) Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chart_finder(page: 'PageObject', chart_selector: str, screenshot: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Encontra gráficos em uma página e retorna os seletores de iframe correspondentes.\n",
    "\n",
    "    Args:\n",
    "        page (PageObject): Objeto da página do Playwright.\n",
    "        chart_selector (str): Seletor CSS para identificar os gráficos na página.\n",
    "        screenshot (bool, optional): Se True, tira uma captura de tela de cada gráfico encontrado. Padrão é True.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de seletores de iframe correspondentes aos gráficos encontrados.\n",
    "    \"\"\"\n",
    "    founded_iframes_selectors = []\n",
    "    founded_charts = await page.query_selector_all(f'.{chart_selector}')\n",
    "\n",
    "    for target in founded_charts:\n",
    "        await target.scroll_into_view_if_needed()\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        title = await target.get_attribute('data-title')\n",
    "        iframe_selector = f'iframe[title=\"{title}\"]'\n",
    "        founded_iframes_selectors.append(iframe_selector)\n",
    "\n",
    "        if screenshot:\n",
    "            page_title = await page.title()\n",
    "            print(f'Page: {page_title} || Chart iframe: {iframe_selector}')\n",
    "            display(Image(await page.screenshot()))\n",
    "\n",
    "    return founded_iframes_selectors\n",
    "\n",
    "ig_iframes = await chart_finder(page=ig, chart_selector=target_data_selector)\n",
    "tk_iframes = await chart_finder(page=tk, chart_selector=target_data_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data Iframe Selector Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iframe in [ig_iframes, tk_iframes]:\n",
    "    print(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iframe_to_remove_1 = 'iframe[title=\"Social App Users\"]'\n",
    "iframe_to_remove_2 = 'iframe[title=\"TikTok Quarterly Downloads\"]'\n",
    "\n",
    "def remove_iframe(iframes):\n",
    "    if iframe_to_remove_1 in iframes:\n",
    "        iframes.remove(iframe_to_remove_1)\n",
    "    if iframe_to_remove_2 in iframes:\n",
    "        iframes.remove(iframe_to_remove_2)\n",
    "    return iframes\n",
    "\n",
    "ig_iframes = remove_iframe(ig_iframes)\n",
    "tk_iframes = remove_iframe(tk_iframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iframe in [ig_iframes, tk_iframes]:\n",
    "    print(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web scraping system located all target data (charts). The results were:\n",
    "- Target data iframes allocated in a list.\n",
    "\n",
    "Web scraping goals reached:\n",
    "\n",
    "1. ~~Configure web scraping stack;~~\t\n",
    "2. ~~Reach target pages;~~\n",
    "3. ~~Locate the charts;~~\n",
    "4. ~~Identify each chart's title;~~\n",
    "5. ~~Construct iframe selectors using the chart titles;~~\n",
    "6. ~~Save the iframe selectors in a list;~~\n",
    "7. Use the iframe list to perform actions within them;\n",
    "8. Hover over each chart column with the mouse pointer;\n",
    "9. Scrape the values that appear in the page source when hovering over each chart column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def barchart_scraper(page: 'PageObject', target_iframes: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    target_iframes: list within iframes target selectors\n",
    "    \"\"\"\n",
    "    scraped_barchart_data = []\n",
    "\n",
    "    print('Webscraping process started')\n",
    "    print(f'Page: {await page.title()}')\n",
    "    print()\n",
    "    for barchart in target_iframes:\n",
    "        barchart_iframe_title = f'{await page.frame_locator(barchart).locator(\"title\").inner_text()}'\n",
    "\n",
    "        columns_qtd = await page.frame_locator(barchart).locator('.igc-graph .igc-column').all()\n",
    "        columns_qtd = len(columns_qtd) + 1\n",
    "\n",
    "        # Mensagem inicial sem a barra de progresso\n",
    "        print(f'-> barchart ({barchart_iframe_title}):', end='')\n",
    "\n",
    "        # Inicialize uma barra de progresso para o gráfico atual\n",
    "        pbar = tqdm(total=columns_qtd - 1, desc='Progress', unit=' Datapoint', leave=True)\n",
    "        \n",
    "        for i in range(1, columns_qtd):\n",
    "            await page.frame_locator(barchart).locator(f'path:nth-child({i})').hover()\n",
    "\n",
    "            target_time_series_category = await page.frame_locator(barchart).locator('.tt_text').inner_text()\n",
    "            target_time_series_value = await page.frame_locator(barchart).locator('.tt_value').inner_text()\n",
    "\n",
    "            # Adicione os dados extraídos à lista\n",
    "            scraped_barchart_data.append({\n",
    "                'iframe_title': barchart_iframe_title,\n",
    "                'timeseries_category': target_time_series_category,\n",
    "                'timeseries_value': target_time_series_value\n",
    "            })\n",
    "\n",
    "            # Atualize a barra de progresso\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Finalize a barra de progresso\n",
    "        pbar.close()\n",
    "\n",
    "        # Imprima a mensagem de conclusão após fechar a barra de progresso\n",
    "        print(f'-> barchart ({barchart_iframe_title}) webscraping ended')\n",
    "        print()\n",
    "    \n",
    "    print('Webscraping process ended')\n",
    "    print('-'*70)\n",
    "    print()\n",
    "\n",
    "    return pd.DataFrame(scraped_barchart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scraped_data = []\n",
    "raw_scraped_data.append(await barchart_scraper(page=ig, target_iframes=ig_iframes))\n",
    "raw_scraped_data.append(await barchart_scraper(page=tk, target_iframes=tk_iframes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Scraped Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quarter_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all dataframes\n",
    "prep_data = pd.concat(raw_scraped_data, ignore_index=True)\n",
    "prep_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = pd.DataFrame(columns=['quarter', 'quarter_label', 'ig_maus', 'ig_revs', 'tk_maus', 'tk_revs'])\n",
    "scraped_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['quarter_label'] = prep_data['timeseries_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter uma string de trimestre para a data de fechamento trimestral \n",
    "def converter_para_data(trimestre):\n",
    "    tri, ano = trimestre.split()\n",
    "    tri = int(tri[1])\n",
    "    if tri == 1:\n",
    "        mes, dia = 3, 31\n",
    "    elif tri == 2:\n",
    "        mes, dia = 6, 30\n",
    "    elif tri == 3:\n",
    "        mes, dia = 9, 30\n",
    "    elif tri == 4:\n",
    "        mes, dia = 12, 31\n",
    "    return pd.Timestamp(year=int(ano), month=mes, day=dia)\n",
    "\n",
    "# Aplicar a transformação ao DataFrame df\n",
    "scraped_data['quarter'] = scraped_data['quarter_label'].apply(converter_para_data)\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = scraped_data.sort_values(by='quarter').reset_index(drop=True)\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o ano atual\n",
    "current_year = pd.Timestamp.now().year\n",
    "\n",
    "# Filtrar os dados para manter apenas os últimos 5 anos\n",
    "scraped_data = scraped_data[scraped_data['quarter'].dt.year >= (current_year - 6)]\n",
    "\n",
    "# Resetar o índice\n",
    "scraped_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Exibir os dados filtrados\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maus and revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para preencher o scraped_data com os valores do prep_data\n",
    "def preencher_scraped_data(prep_data, scraped_data):\n",
    "    for index, row in prep_data.iterrows():\n",
    "        if 'Instagram' in row['iframe_title']:\n",
    "            if 'monthly app users' in row['iframe_title']:\n",
    "                scraped_data.loc[scraped_data['quarter_label'] == row['timeseries_category'], 'ig_maus'] = row['timeseries_value']\n",
    "            elif 'revenues' in row['iframe_title']:\n",
    "                scraped_data.loc[scraped_data['quarter_label'] == row['timeseries_category'], 'ig_revs'] = row['timeseries_value']\n",
    "        elif 'TikTok' in row['iframe_title']:\n",
    "            if 'MAUs' in row['iframe_title']:\n",
    "                scraped_data.loc[scraped_data['quarter_label'] == row['timeseries_category'], 'tk_maus'] = row['timeseries_value']\n",
    "            elif 'revenues' in row['iframe_title']:\n",
    "                scraped_data.loc[scraped_data['quarter_label'] == row['timeseries_category'], 'tk_revs'] = row['timeseries_value']\n",
    "    return scraped_data\n",
    "\n",
    "# Preencher o scraped_data\n",
    "scraped_data = preencher_scraped_data(prep_data, scraped_data)\n",
    "scraped_data = scraped_data.astype({'ig_maus': 'int64', 'ig_revs': 'int64', 'tk_maus': 'int64', 'tk_revs': 'int64'})\n",
    "\n",
    "# Exibir o resultado\n",
    "display(scraped_data.head())\n",
    "display(scraped_data.tail())\n",
    "display(scraped_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraped Data Exporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_sha256_hash(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Gera uma hash SHA256 para um DataFrame.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(data.to_string().encode())\n",
    "    hash = hash_object.hexdigest()\n",
    "    print('Hash do DataFrame carregado:', hash)\n",
    "\n",
    "def data_integrity_test(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Para garantir a reprodutibilidade deste notebook, o hash SHA-256 do DataFrame em estado raw (sem transformações) deve ser exatamente:\n",
    "    50f08805a0b891323bc75e43010e3622e80a2607c8cf585729b153f7703adf6f\n",
    "\n",
    "    Esta função garante a integridade dos dados. Se os dados estiverem íntegros, um output de conformidade será exibido.\n",
    "    Caso contrário, o notebook lançará um erro, indicando que o experimento original não é reprodutível.\n",
    "    \"\"\"\n",
    "    ex_hash = '50f08805a0b891323bc75e43010e3622e80a2607c8cf585729b153f7703adf6f'\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(data.to_string().encode())\n",
    "    hash = hash_object.hexdigest()\n",
    "\n",
    "    print('Hash do DataFrame esperado: ',ex_hash)\n",
    "\n",
    "    if hash == ex_hash:\n",
    "        print('-> DataFrame carregado passou no teste de intrigade. Hashes constaram como idênticas.')\n",
    "        print('-> Experimento/análise original pode ser reproduzido.')\n",
    "\n",
    "    else:\n",
    "        raise ValueError('DataFrame carregado NÃO passou no teste de intrigade. O experimento/análise original NÃO pode ser reproduzido.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as colunas numéricas para int\n",
    "scraped_data[['ig_maus', 'ig_revs', 'tk_maus', 'tk_revs']] = scraped_data[['ig_maus', 'ig_revs', 'tk_maus', 'tk_revs']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove a ultima linha do df\n",
    "scraped_data = scraped_data[:-1]\n",
    "scraped_data.to_parquet('data/raw/scraped_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
